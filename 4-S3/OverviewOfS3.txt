# Overview of S3 - Simple Storage Services

Amazon S3 supports the largest file size upto 5TB.
The service operates on object storage system.
S3 is a regional service. While uploading the data, we are required to specify the
location in which AWS region we want to upload the data.
It will then upload the data multiple times across multiple AZs thereby increasing
the durability , availability and reducing the latency.

Amazon S3 offers durability of 99.999999999%
The availability of S3 objects depends on the storage class used, therefore, it ranges
from 99.5% to 99.99%

To store data in Amazon S3, we need to define a S3 bucket.
the name of the bucket should be unique. Since the buckets are created globally,
this is because of the flat address space.

By default, an AWS account can have 100 buckets and this is a soft limit.

S3 CORS:
-  CORS is abbreviation for Cross Origin Resource Sharing.

S3 Default Encryption:
-  We can enable the default encryption option while creating/modifying a bucket.

S3 Cross Region and Same Region Replication:
CRR:
  - Versioning must be enabled in both source and destination bucket.
  - Asynchronous replication happens between two different region buckets.
  - Buckets can be in different AWS accounts as well.
  - Must give proper IAM permission to S3.
  - After activating, only new objects will be replicated.
  - For replicating existing objects in the bucket, we can use S3 Batch Replication.
  - For DELETE options: we can replicate delete marker of the objects from source to target bucket.
  - There is no chaining of replication.
  - Permanent delete on objects are not replicated.

S3 Pre-Signed URLs:
-  Can generate pre-signed URLs using SDK or CLIs.
-  They have a default TTL of 3600 seconds. It is possible to change the timeout using following argument --expires-in
   [TIME_IN_SECONDS] argument.
-  Users given a pre-signed URL inherit the permissions of user who generated the URL.

S3 baseline performance:
-  Your application can achieve at least 3500 PUT/DELETE/COPY/POST per second and 5,500 GET/HEAD requests per second
   per prefix in a bucket.
- If a bucket has 4 folders, then we can say it has 4 prefixes and the total GET/HEADs/second would be 22,000.

S3 Bytes ranges fetch:
-  Parallelize GETs by requesting specific Bytes ranges of the file.
-  Better resilience in case of failures.
-  Used for speeding up downloads
- Another use case: Can be used to retrieve only partial file.

S3 Select and Glacier Select:
-  Using S3 select we can retrieve fewer data using SQL by performing server side filtering
-  Can filter by rows and columns.
-  Less network transfer, less CPU cost client-side
- Upto 400% faster and 80% cheaper

S3 Event Notifications:
-  Any object related notification which can be sent to SNS, SQS, Lambda and Amazon EventBridge.

S3 Requester Pays:
-  With Requester Pays bucket, the requester will pay for downloading objects from bucket, rather than bucket owner.
-  Here, Requester will pay for the networking cost and owner will only bear storage cost.
-  The requester will have to a AWS authenticated user so that amazon can bill them.

Amazon Athena:
-  Serverless query service to perform analytics on S3 objects.
-  Amazon Athena can then be integrated with Amazon QuickSight for some dashboards.
-  Pricing is pay per query or $5 per TB for data scanned.
-  Use compressed or columnar data for cost savings.

Glacier Vault Lock:
-  Adopt a WORM (Write Once Read Many) policy.
-  Lock the policy for future edits.
-  Helpful for data and compliance retention.
-  Once the data is written to Glacier Vault, it cannot be tampered when this policy is enabled.